{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from utils import Kron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TupleToString(tup):\n",
    "    # Convert a tuple to a string where each number is assigned to its string form.\n",
    "    return \"\".join(list(map(lambda i: string.printable[10 + i], tup)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YieldScalar(tndict):\n",
    "    # Check if contracting all the tensors in the dictionary yields a scalar.\n",
    "    supports = np.array([np.array(list(support), dtype = np.int) for (support, __) in tndict], dtype = np.int).flatten()\n",
    "    #print(\"supports = {}\".format(supports))\n",
    "    visited = np.zeros(supports.shape[0], dtype = np.int)\n",
    "    for i in range(supports.size):\n",
    "        not_scalar = 1\n",
    "        for j in range(supports.size):\n",
    "            if (i != j):\n",
    "                if (supports[i] == supports[j]):\n",
    "                    not_scalar = 0\n",
    "                    #print(\"Comparing support[%d] = %d and support[%d] = %d.\" % (i, supports[i], j, supports[j])) \n",
    "    scalar = 1 - not_scalar\n",
    "    return scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef TensorTrace(tndict, opt = \"greedy\"):\\n    # Contract a set of tensors provided as a dictionary.\\n    # The result of the traction must be a number, in other words, there should be no free indices.\\n    if (YieldScalar(tndict) == 0):\\n        print(\"The Tensor contraction does not result in a trace.\")\\n        return None\\n    # We want to use np.einsum(...) to perform the Tensor contraction.\\n    # Every element of the input dictionary is a tuple, containing the support of a tensor and its operator form.\\n    # Numpy\\'s einsum function simply needs the Tensors and the corresponding labels associated to its indices.\\n    # We will convert the support of each Tensor to a string, that serves as its label.\\n    scheme = \",\".join([TupleToString(support) for (support, __) in tndict])\\n    print(\"Contraction scheme: {}\".format(scheme))\\n    ops = [op for (__, op) in tndict]\\n    ops_args = \", \".join([(\"ops[%d]\" % d) for d in range(len(ops))])\\n    print(\"np.einsum_path(\\'%s->\\', %s, optimize=\\'%s\\')\" % (scheme, ops_args, opt))\\n    path = eval(\"np.einsum_path(\\'%s->\\', %s, optimize=\\'%s\\')\" % (scheme, ops_args, opt))\\n    print(\"Contraction process\\n{}: {}\\n{}\".format(path[0][0], path[0][1:], path[1]))\\n    trace = np.einsum(scheme, *ops, optimize=path[0])\\n    #print(\"Trace = {}.\".format(trace))\\n    return trace\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def TensorTrace(tndict, opt = \"greedy\"):\n",
    "    # Contract a set of tensors provided as a dictionary.\n",
    "    # The result of the traction must be a number, in other words, there should be no free indices.\n",
    "    if (YieldScalar(tndict) == 0):\n",
    "        print(\"The Tensor contraction does not result in a trace.\")\n",
    "        return None\n",
    "    # We want to use np.einsum(...) to perform the Tensor contraction.\n",
    "    # Every element of the input dictionary is a tuple, containing the support of a tensor and its operator form.\n",
    "    # Numpy's einsum function simply needs the Tensors and the corresponding labels associated to its indices.\n",
    "    # We will convert the support of each Tensor to a string, that serves as its label.\n",
    "    scheme = \",\".join([TupleToString(support) for (support, __) in tndict])\n",
    "    print(\"Contraction scheme: {}\".format(scheme))\n",
    "    ops = [op for (__, op) in tndict]\n",
    "    ops_args = \", \".join([(\"ops[%d]\" % d) for d in range(len(ops))])\n",
    "    print(\"np.einsum_path(\\'%s->\\', %s, optimize=\\'%s\\')\" % (scheme, ops_args, opt))\n",
    "    path = eval(\"np.einsum_path(\\'%s->\\', %s, optimize=\\'%s\\')\" % (scheme, ops_args, opt))\n",
    "    print(\"Contraction process\\n{}: {}\\n{}\".format(path[0][0], path[0][1:], path[1]))\n",
    "    trace = np.einsum(scheme, *ops, optimize=path[0])\n",
    "    #print(\"Trace = {}.\".format(trace))\n",
    "    return trace\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, 2, 3), (2, 3, 4, 5), (4, 5, 6, 7), (6, 7, 8, 9), (8, 9, 10, 11), (10, 11, 12, 13), (12, 13, 14, 15), (14, 15, 16, 17), (16, 17, 18, 19), (18, 19, 20, 21), (20, 21, 22, 23), (22, 23, 0, 1)]\n"
     ]
    }
   ],
   "source": [
    "N = 24\n",
    "dims = 4\n",
    "bond = 4\n",
    "# create some numpy tensors\n",
    "tensors = [np.random.rand(*[bond]*dims) for __ in range(0, N, 2)]\n",
    "# labels should ensure that there is no free index.\n",
    "labels = [tuple([(i + j) % N for j in range(4)]) for i in range(0, N, 2)]\n",
    "print(labels)\n",
    "# Prepare the dictionary.\n",
    "tndict = [(labels[i], tensors[i]) for i in range(len(labels))]\n",
    "#print([tnop for (lab, tnop) in tndict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trace = TensorTrace(tndict, opt=\"greedy\")\n",
    "# print(\"Trace = {}.\".format(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimalEinsum(scheme, ops, opt = \"greedy\"):\n",
    "    # Contract a tensor network using einsum supplemented with its optimization tools.\n",
    "    ops_args = \", \".join([(\"ops[%d]\" % d) for d in range(len(ops))])\n",
    "    #print(\"Calling np.einsum({}, {})\\nwhere shapes are\\n{}.\".format(scheme, ops_args, [op.shape for op in ops]))\n",
    "    path = eval(\"np.einsum_path(\\'%s\\', %s, optimize=\\'%s\\')\" % (scheme, ops_args, opt))\n",
    "    #print(\"Contraction process\\n{}: {}\\n{}\".format(path[0][0], path[0][1:], path[1]))\n",
    "    prod = np.einsum(scheme, *ops, optimize=path[0])\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorTranspose(tensor):\n",
    "    # Transpose the tensor, in other words, exchange its row and column indices.\n",
    "    # Note that when we reshape a matrix into (D, D, ..., D) tensor, it stores the indices as as\n",
    "    # row_1, row_2, row_3, ..., row_(D/2), col_1, ..., col_(D/2).\n",
    "    rows = range(0, tensor.ndim//2)\n",
    "    cols = range(tensor.ndim//2, tensor.ndim)\n",
    "    tp_indices = np.concatenate((cols, rows))\n",
    "    return np.transpose(tensor, tp_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ?= A.T is True\n"
     ]
    }
   ],
   "source": [
    "# Testing TensorTranspose\n",
    "nq = 2\n",
    "tensor = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "tp_tensor = TensorTranspose(tensor)\n",
    "#print(\"tensor\\n{}\\nand its transpose\\n{}\".format(tensor.reshape(2**dims, 2**dims), tp_tensor.reshape(2**dims, 2**dims)))\n",
    "print(\"A ?= A.T is {}\".format(np.allclose(tensor.reshape(2**nq, 2**nq), tp_tensor.reshape(2**nq, 2**nq).T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNQubitPauli(ind, nq):\n",
    "    # Compute the n-qubit Pauli that is at position 'i' in an ordering based on [I, X, Y, Z].\n",
    "    # We will express the input number in base 4^n - 1.\n",
    "    pauli = np.zeros(nq, dtype = np.int)\n",
    "    for i in range(nq):\n",
    "        pauli[i] = ind % 4\n",
    "        ind = int(ind//4)\n",
    "    return pauli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing GetNQubitPauli\n",
    "GetNQubitPauli(172, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorKron(tn1, tn2):\n",
    "    # Compute the Kronecker product of two tensors A and B.\n",
    "    # This is not equal to np.tensordot(A, B, axis = 0), see: https://stackoverflow.com/questions/52125078/why-does-tensordot-reshape-not-agree-with-kron.\n",
    "    # Note that when we reshape a matrix into (D, D, ..., D) tensor, it stores the indices as as\n",
    "    # row_1, row_2, row_3, ..., row_(D/2), col_1, ..., col_(D/2).\n",
    "    # We will implement Kronecker product using einsum, as\n",
    "    # np.einsum('rA1 rA2 .. rAn, cA1 cA2 .. cAn, rB1 rB2 .. rBn, cB1 cB2 .. cBn -> rA1 rB1 rA2 rB2 .. rAn rBn, cA1 cB1 cA2 cB2 .. cAn cBn', A, B).\n",
    "    if (tn1.ndim != tn2.ndim):\n",
    "        print(\"TensorKron does not work presently for tensors of different dimensions.\")\n",
    "        return None\n",
    "    tn1_rows = [string.printable[10 + i] for i in range(tn1.ndim//2)]\n",
    "    tn1_cols = [string.printable[10 + i] for i in range(tn1.ndim//2, tn1.ndim)]\n",
    "    tn2_rows = [string.printable[10 + tn1.ndim + i] for i in range(tn2.ndim//2)]\n",
    "    tn2_cols = [string.printable[10 + tn1.ndim + i] for i in range(tn2.ndim//2, tn2.ndim)]\n",
    "    #kron_inds = [\"%s%s\" % (tn1_rows[i], tn2_rows[i]) for i in range(tn1.ndim//2)]\n",
    "    #kron_inds += [\"%s%s\" % (tn1_cols[i], tn2_cols[i]) for i in range(tn1.ndim//2)]\n",
    "    kron_inds = [\"%s\" % (tn1_rows[i]) for i in range(tn1.ndim//2)]\n",
    "    kron_inds += [\"%s\" % (tn2_rows[i]) for i in range(tn1.ndim//2)]\n",
    "    kron_inds += [\"%s\" % (tn1_cols[i]) for i in range(tn1.ndim//2)]\n",
    "    kron_inds += [\"%s\" % (tn2_cols[i]) for i in range(tn1.ndim//2)]\n",
    "    scheme = (\"%s%s,%s%s->%s\" % (\"\".join(tn1_rows), \"\".join(tn1_cols), \"\".join(tn2_rows), \"\".join(tn2_cols), \"\".join(kron_inds)))\n",
    "    return OptimalEinsum(scheme, [tn1, tn2], opt = \"greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorKron(A, B) ?= A o B is True\n"
     ]
    }
   ],
   "source": [
    "# Testing TensorKron\n",
    "nq = 4\n",
    "tn1 = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "tn2 = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "kn_tensor = TensorKron(tn1, tn2)\n",
    "#print(\"A\\n{}\\nB\\n{}\\nA o B = \\n{}\".format(tn1.reshape(2**nq, 2**nq), tn2.reshape(2**nq, 2**nq), kn_tensor.reshape(4**nq, 4**nq)))\n",
    "print(\"TensorKron(A, B) ?= A o B is {}\".format(np.allclose(kn_tensor.reshape(4**nq, 4**nq), np.kron(tn1.reshape(2**nq, 2**nq), tn2.reshape(2**nq, 2**nq)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TraceDot(tn1, tn2):\n",
    "    # Compute the trace of the dot product of two tensors A and B.\n",
    "    # If the indices of A are i_0 i_1, ..., i_(2n-1) and that of B are j_0 j_1 ... j_(2n-1)\n",
    "    # then want to contract the indices i_(2k) with j_(2k+1), for all k in [0, n-1].\n",
    "    # While calling np.einsum, we need to ensure that the row index of A is equal to the column index of B.\n",
    "    # Additionally to ensure that we have a trace, we need to match the row and column indices of the product.\n",
    "    tn1_rows = [string.printable[10 + i] for i in range(tn1.ndim//2)]\n",
    "    tn1_cols = [string.printable[10 + i] for i in range(tn1.ndim//2, tn1.ndim)]\n",
    "    # The column indices of tn1 should match row indices of tn2\n",
    "    # So, tn1_cols = tn2_rows.\n",
    "    # the row and column indices of the product must match\n",
    "    # So, tn1_rows = tn2_cols.\n",
    "    scheme = (\"%s%s,%s%s->\" % (\"\".join(tn1_rows), \"\".join(tn1_cols), \"\".join(tn1_cols), \"\".join(tn1_rows)))\n",
    "    return OptimalEinsum(scheme, [tn1, tn2], opt = \"greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TraceDot(A, B) ?= Tr(A . B) is True.\n"
     ]
    }
   ],
   "source": [
    "# Testing TraceDot\n",
    "nq = 5\n",
    "tn1 = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "tn2 = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "trdot = TraceDot(tn1, tn2)\n",
    "nptr = np.trace(np.dot(tn1.reshape(2**nq, 2**nq), tn2.reshape(2**nq, 2**nq)))\n",
    "#print(\"A\\n{}\\nB\\n{}\\nTr(A . B) = \\n{}\".format(tn1.reshape(2**nq, 2**nq), tn2.reshape(2**nq, 2**nq), trdot))\n",
    "#print(\"TensorTrace = {}\\nNumpy Trace = {}\".format(trdot, nptr))\n",
    "print(\"TraceDot(A, B) ?= Tr(A . B) is {}.\".format(np.allclose(trdot, nptr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PauliTensor(pauli_op):\n",
    "    # Convert a Pauli in operator form to a tensor.\n",
    "    # The tensor product of A B .. Z is given by simply putting the rows indices of A, B, ..., Z together, followed by their column indices.\n",
    "    # Each qubit index q can be assigned a pair of labels for the row and columns of the Pauli matrix on q: C[2q], C[2q + 1].\n",
    "    # Pauli matrices\n",
    "    characters = string.printable[10:]\n",
    "    # replace the following line with the variable from globalvars.py\n",
    "    Pauli = np.array([[[1, 0], [0, 1]], [[0, 1], [1, 0]], [[0, -1j], [1j, 0]], [[1, 0], [0, -1]]], dtype=np.complex128)\n",
    "    nq = pauli_op.shape[0]\n",
    "    labels = \",\".join([\"%s%s\" % (characters[2 * q], characters[2 * q + 1]) for q in range(nq)])\n",
    "    ops = [Pauli[pauli_op[q], :, :] for q in range(nq)]\n",
    "    kn_indices = [\"%s\" % (characters[2 * q]) for q in range(nq)]\n",
    "    kn_indices += [\"%s\" % (characters[2 * q + 1]) for q in range(nq)]\n",
    "    kn_label = \"\".join(kn_indices)\n",
    "    scheme = \"%s->%s\" % (labels, kn_label)\n",
    "    pauli_tensor = OptimalEinsum(scheme, ops)\n",
    "    return pauli_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pauli operator: [2 2 2]\n",
      "PauliTensor - Numpy = True\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "pauli_op = np.random.randint(0, high=4, size=(N,))\n",
    "print(\"Pauli operator: {}\".format(pauli_op))\n",
    "tn_pauli = PauliTensor(pauli_op)\n",
    "Pauli = np.array([[[1, 0], [0, 1]], [[0, 1], [1, 0]], [[0, -1j], [1j, 0]], [[1, 0], [0, -1]]], dtype=np.complex128)\n",
    "np_pauli = Kron(*Pauli[pauli_op, :, :])\n",
    "print(\"PauliTensor - Numpy = {}\".format(np.allclose(tn_pauli.reshape(2**N, 2**N), np_pauli)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KraussToTheta(kraus):\n",
    "    # Convert from the Kraus representation to the \"Theta\" representation.\n",
    "    # The \"Theta\" matrix T of a CPTP map whose chi-matrix is X is defined as:\n",
    "    # T_ij = \\sum_(ij) [ X_ij (P_i o (P_j)^T) ]\n",
    "    # Note that the chi matrix X can be defined using the Kraus matrices {K_k} in the following way\n",
    "    # X_ij = \\sum_k [ <P_i|K_k><K_k|P_j> ]\n",
    "    # \t   = \\sum_k [ Tr(P_i K_k) Tr((K_k)^\\dag P_j)]\n",
    "    # So we find that\n",
    "    # T = \\sum_(ij) [ \\sum_k [ Tr(P_i K_k) Tr((K_k)^\\dag P_j)] ] (P_i o (P_j)^T) ]\n",
    "    # We will store T as a Tensor with dimension = (2 * number of qubits) and bond dimension = 4.\n",
    "    nq = int(np.log2(kraus.shape[1]))\n",
    "    theta = np.zeros(tuple([4, 4]*nq), dtype = np.complex128)\n",
    "    for i in range(4**nq):\n",
    "        for j in range(4**nq):\n",
    "            Pi = PauliTensor(GetNQubitPauli(i, nq))\n",
    "            Pj = PauliTensor(GetNQubitPauli(j, nq))\n",
    "            PjT = TensorTranspose(Pj)\n",
    "            PioPjT = np.reshape(TensorKron(Pi, PjT), tuple([4, 4] * nq))\n",
    "            #print(\"Pi.shape = {}\\nPi\\n{}\".format(Pi.shape, Pi))\n",
    "            #print(\"Pj.shape = {}\\nPj\\n{}\".format(Pj.shape, Pj))\n",
    "            chi_ij = 0 + 0 * 1j\n",
    "            for k in range(kraus.shape[0]):\n",
    "                K = np.reshape(kraus[k, :, :], tuple([2, 2]*nq))\n",
    "                Kdag = np.conj(TensorTranspose(K))\n",
    "                #print(\"K.shape = {}\\nK\\n{}\".format(K.shape, K))\n",
    "                #print(\"TraceDot(K, Pi) = {}\".format(TraceDot(K, Pi)))\n",
    "                #print(\"Kdag.shape = {}\\nKdag\\n{}\".format(Kdag.shape, Kdag))\n",
    "                #print(\"Pj.shape = {}\\nPj\\n{}\".format(Pj.shape, Pj))\n",
    "                #print(\"TraceDot(Kdag, Pj) = {}\".format(TraceDot(K, Pj)))\n",
    "                chi_ij += TraceDot(Pi, K) * TraceDot(Pj, Kdag)\n",
    "            chi_ij /= 4**nq\n",
    "            #print(\"Chi[%d, %d] = %g + i %g\" % (i, j, np.real(coeff), np.imag(coeff)))\n",
    "            theta += chi_ij * PioPjT\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta\n",
      "[[0.93333333+0.j 0.        +0.j 0.        +0.j 0.06666667+0.j]\n",
      " [0.        +0.j 0.86666667+0.j 0.        +0.j 0.        +0.j]\n",
      " [0.        +0.j 0.        +0.j 0.86666667+0.j 0.        +0.j]\n",
      " [0.06666667+0.j 0.        +0.j 0.        +0.j 0.93333333+0.j]]\n"
     ]
    }
   ],
   "source": [
    "# depolarizing channel\n",
    "N = 1\n",
    "Pauli = np.array([[[1, 0], [0, 1]], [[0, 1], [1, 0]], [[0, -1j], [1j, 0]], [[1, 0], [0, -1]]], dtype=np.complex128)\n",
    "kraus_dp = np.zeros((4**N, 2**N, 2**N), dtype = np.complex128)\n",
    "rate = 0.1\n",
    "kraus_dp[0, :, :] = np.sqrt(1 - rate) * Pauli[0, :, :]\n",
    "for k in range(1, 4):\n",
    "    kraus_dp[k, :, :] = np.sqrt(rate/3) * Pauli[k, :, :]\n",
    "theta = KraussToTheta(kraus_dp)\n",
    "print(\"Theta\\n{}\".format(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SupportToLabel(supports, characters = None):\n",
    "    # Convert a list of qubit indices to labels for a tensor.\n",
    "    # Each qubit index corresponds to a pair of labels, indicating the row and column indices of the 2 x 2 matrix which acts non-trivially on that qubit.\n",
    "    # Each number in the support is mapped to a pair of alphabets in the characters list, as: x -> (characters[2x], characters[2x + 1]).\n",
    "    # Eg. (x, y, z) ---> (C[2x] C[2y] C[2z] , C[2x + 1] C[2y + 1] C[2z + 1])\n",
    "    if characters == None:\n",
    "        characters = [c for c in string.ascii_lowercase] + [c for c in string.ascii_uppercase]\n",
    "    #print(\"characters\\n{}\".format(characters))\n",
    "    #print(\"support\\n{}\".format(supports))\n",
    "    labels = [[[-1, -1] for q in interac] for interac in supports]\n",
    "    #print(\"labels\\n{}\".format(labels))\n",
    "    unique_qubits = np.unique([q for sup in supports for q in sup])\n",
    "    #print(\"unique qubits\\n{}\".format(unique_qubits))\n",
    "    free_index = {q:[-1, -1] for q in unique_qubits}\n",
    "    for i in range(len(supports)):\n",
    "            sup = supports[i]\n",
    "            #print(\"Support: {}\".format(sup))\n",
    "            for j in range(len(sup)):\n",
    "                    #print(\"Qubit: {}\".format(sup[j]))\n",
    "                    q = sup[j]\n",
    "                    if (free_index[q][0] == -1):\n",
    "                        free_index[q][0] = characters.pop()\n",
    "                        free_index[q][1] = characters.pop()\n",
    "                        #print(\"Assigning {} and {} to qubit {} of map {}\\n\".format(free_index[q][0],free_index[q][1],q,i))\n",
    "                        labels[i][j][0] = free_index[q][0]\n",
    "                        labels[i][j][1] = free_index[q][1]\n",
    "                    else:\n",
    "                        labels[i][j][0] = free_index[q][1]\n",
    "                        free_index[q][1] = characters.pop()\n",
    "                        labels[i][j][1] = free_index[q][1]\n",
    "                        #print(\"Assigning {} and {} to qubit {} of map {}\\n\".format(labels[i][j][0],labels[i][j][1],q,i))\n",
    "                    #print(\"labels\\n{}\\nfree index\\n{}\".format(labels, free_index))\n",
    "    #print(\"labels\\n{}\\nfree index\\n{}\".format(labels, free_index))\n",
    "    return (labels, free_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ContractThetaNetwork(theta_dict, MAX = 10):\n",
    "    # Compute the Theta matrix of a composition of channels.\n",
    "    # The individual channels are provided a list where each one is a pair: (s, O) where s is the support and O is the theta matrix.\n",
    "    # We will use einsum to contract the tensor network of channels.\n",
    "    supports = [list(sup) for (sup, op) in theta_dict]\n",
    "    if (len(supports) > MAX):\n",
    "        partial_network = theta_dict[:MAX]\n",
    "        partial_contraction = ContractThetaNetwork(partial_network)\n",
    "        remaining_network = theta_dict[MAX:]\n",
    "        remaining_contraction = ContractThetaNetwork(remaining_network)\n",
    "        return ContractThetaNetwork(partial_contraction + remaining_contraction)     \n",
    "    (contraction_labels, free_labels) = SupportToLabel(supports)\n",
    "    #print(\"contraction_labels = {}\".format(contraction_labels))\n",
    "    row_labels = [\"\".join([q[0] for q in interac]) for interac in contraction_labels]\n",
    "    #print(\"row_contraction_labels = {}\".format(row_labels))\n",
    "    col_labels = [\"\".join([q[1] for q in interac]) for interac in contraction_labels]\n",
    "    #print(\"col_contraction_labels = {}\".format(col_labels))\n",
    "    left = [\"%s%s\" % (row_labels[i], col_labels[i]) for i in range(len(contraction_labels))]\n",
    "    #print(\"left = {}\".format(left))\n",
    "    contraction_scheme = \"%s\" % (\",\".join(left))\n",
    "    #print(\"contraction_scheme = {}\".format(contraction_scheme))\n",
    "    free_row_labels = [free_labels[q][0] for q in free_labels]\n",
    "    #print(\"free_row_labels = {}\".format(free_row_labels))\n",
    "    free_col_labels = [free_labels[q][1] for q in free_labels]\n",
    "    #print(\"free_col_labels = {}\".format(free_col_labels))\n",
    "    contracted_labels = \"%s%s\" % (\"\".join(free_row_labels), \"\".join(free_col_labels))\n",
    "    #print(\"contracted_labels = {}\".format(contracted_labels))\n",
    "    scheme = \"%s->%s\" % (contraction_scheme, contracted_labels)\n",
    "    #print(\"Contraction scheme = {}\".format(scheme))\n",
    "    theta_ops = [op for (__, op) in theta_dict]\n",
    "    composed = OptimalEinsum(scheme, theta_ops)\n",
    "    composed_support = np.unique([q for (sup, op) in theta_dict for q in sup])\n",
    "    composed_dict = [(composed_support, composed)]\n",
    "    return composed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result supported on [0 1 2 3] has dimensions (4, 4, 4, 4, 4, 4, 4, 4).\n"
     ]
    }
   ],
   "source": [
    "theta_dict = [(range(4), np.random.rand(4,4,4,4,4,4,4,4)), ((0,1), np.random.rand(4,4,4,4)), ((1,2), np.random.rand(4,4,4,4)), ((2,3), np.random.rand(4,4,4,4))]\n",
    "#print(\"Contracting the Theta network\\n{}\".format(theta_dict))\n",
    "(contracted_support, contracted_split) = ContractThetaNetwork(theta_dict, MAX=2)[0]\n",
    "print(\"Result supported on {} has dimensions {}.\".format(contracted_support, contracted_split.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result supported on [0 1 2 3] has dimensions (4, 4, 4, 4, 4, 4, 4, 4).\n"
     ]
    }
   ],
   "source": [
    "(contracted_support, contracted_all) = ContractThetaNetwork(theta_dict, MAX=4)[0]\n",
    "print(\"Result supported on {} has dimensions {}.\".format(contracted_support, contracted_all.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.8421709430404007e-13"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(contracted_all - contracted_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorTrace(tensor, indices = \"all\", characters = None):\n",
    "    # Compute the Trace of the tensor.\n",
    "    if characters == None:\n",
    "        characters = [c for c in string.ascii_lowercase] + [c for c in string.ascii_uppercase]\n",
    "    labels = [characters[i] for i in range(tensor.ndim)]\n",
    "    if indices == \"all\":\n",
    "        indices = range(tensor.ndim//2)\n",
    "    for i in indices:\n",
    "        labels[i] = labels[i + int(tensor.ndim//2)]\n",
    "    # Find unique labels in labels.\n",
    "    print(\"labels = {}\".format(labels))\n",
    "    (right, counts) = np.unique(labels, return_counts=True)\n",
    "    free_labels = list(right[np.argwhere(counts == 1).flatten()])\n",
    "    scheme = \"%s->%s\" % (\"\".join(labels), \"\".join(free_labels))\n",
    "    trace = OptimalEinsum(scheme, [tensor])\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ThetaToChiElement(pauli_op_i, pauli_op_j, theta, supp_theta):\n",
    "    # Convert from the Theta representation to the Chi representation.\n",
    "    # The \"Theta\" matrix T of a CPTP map whose chi-matrix is X is defined as:\n",
    "    # T_ij = \\sum_(ij) [ X_ij (P_i o (P_j)^T) ]\n",
    "    # So we find that\n",
    "    # Chi_ij = Tr[ (P_i o (P_j)^T) T]\n",
    "    # We will store T as a Tensor with dimension = (2 * number of qubits) and bond dimension = 4.\n",
    "    nq = pauli_op_i.size\n",
    "    print(\"nq = {}\".format(nq))\n",
    "    Pi = PauliTensor(pauli_op_i)\n",
    "    print(\"Pi shape = {}\".format(Pi.shape))\n",
    "    Pj = PauliTensor(pauli_op_j)\n",
    "    print(\"Pj shape = {}\".format(Pj.shape))\n",
    "    PjT = TensorTranspose(Pj)\n",
    "    print(\"PjT shape = {}\".format(PjT.shape))\n",
    "    PioPjT = np.reshape(TensorKron(Pi, PjT), tuple([4, 4] * nq))\n",
    "    print(\"PioPjT shape = {}\".format(PioPjT.shape))\n",
    "    theta_reshaped = theta.reshape(*[4,4] * len(supp_theta))\n",
    "    print(\"theta_reshaped shape = {}\".format(theta_reshaped.shape))\n",
    "    (__, PioPjT_theta) = ContractThetaNetwork([(tuple(list(range(nq))), PioPjT), (supp_theta, theta_reshaped)])[0]\n",
    "    print(\"PioPjT_theta shape = {}.\".format(PioPjT_theta.shape))\n",
    "    chi_elem = TensorTrace(PioPjT_theta)\n",
    "    return chi_elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nq = 4\n",
      "Pi shape = (2, 2, 2, 2, 2, 2, 2, 2)\n",
      "Pj shape = (2, 2, 2, 2, 2, 2, 2, 2)\n",
      "PjT shape = (2, 2, 2, 2, 2, 2, 2, 2)\n",
      "PioPjT shape = (4, 4, 4, 4, 4, 4, 4, 4)\n",
      "theta_reshaped shape = (4, 4, 4, 4)\n",
      "PioPjT_theta shape = (4, 4, 4, 4, 4, 4, 4, 4).\n",
      "labels = ['e', 'f', 'g', 'h', 'e', 'f', 'g', 'h']\n",
      "chi_ij = (117.04895094495778+0j)\n"
     ]
    }
   ],
   "source": [
    "pauli_op_i = GetNQubitPauli(0, 4)\n",
    "pauli_op_j = GetNQubitPauli(0, 4)\n",
    "theta = np.random.rand(16, 16)\n",
    "supp_theta = (0, 1)\n",
    "chi_ij = ThetaToChiElement(pauli_op_i, pauli_op_j, theta, supp_theta)\n",
    "print(\"chi_ij = {}\".format(chi_ij))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pauli_op_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
