{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bc'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def TupleToString(tup):\n",
    "    # Convert a tuple to a string where each number is assigned to its string form.\n",
    "    return \"\".join(list(map(lambda i: string.printable[10 + i], tup)))\n",
    "TupleToString((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YieldScalar(tndict):\n",
    "    # Check if contracting all the tensors in the dictionary yields a scalar.\n",
    "    supports = np.array([np.array(list(support), dtype = np.int) for (support, __) in tndict], dtype = np.int).flatten()\n",
    "    #print(\"supports = {}\".format(supports))\n",
    "    visited = np.zeros(supports.shape[0], dtype = np.int)\n",
    "    for i in range(supports.size):\n",
    "        not_scalar = 1\n",
    "        for j in range(supports.size):\n",
    "            if (i != j):\n",
    "                if (supports[i] == supports[j]):\n",
    "                    not_scalar = 0\n",
    "                    #print(\"Comparing support[%d] = %d and support[%d] = %d.\" % (i, supports[i], j, supports[j])) \n",
    "    scalar = 1 - not_scalar\n",
    "    return scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorTrace(tndict, opt = \"greedy\"):\n",
    "    # Contract a set of tensors provided as a dictionary.\n",
    "    # The result of the traction must be a number, in other words, there should be no free indices.\n",
    "    if (YieldScalar(tndict) == 0):\n",
    "        print(\"The Tensor contraction does not result in a trace.\")\n",
    "        return None\n",
    "    # We want to use np.einsum(...) to perform the Tensor contraction.\n",
    "    # Every element of the input dictionary is a tuple, containing the support of a tensor and its operator form.\n",
    "    # Numpy's einsum function simply needs the Tensors and the corresponding labels associated to its indices.\n",
    "    # We will convert the support of each Tensor to a string, that serves as its label.\n",
    "    scheme = \",\".join([TupleToString(support) for (support, __) in tndict])\n",
    "    print(\"Contraction scheme: {}\".format(scheme))\n",
    "    ops = [op for (__, op) in tndict]\n",
    "    ops_args = \", \".join([(\"ops[%d]\" % d) for d in range(len(ops))])\n",
    "    print(\"np.einsum_path(\\'%s->\\', %s, optimize=\\'%s\\')\" % (scheme, ops_args, opt))\n",
    "    path = eval(\"np.einsum_path(\\'%s->\\', %s, optimize=\\'%s\\')\" % (scheme, ops_args, opt))\n",
    "    print(\"Contraction process\\n{}: {}\\n{}\".format(path[0][0], path[0][1:], path[1]))\n",
    "    trace = np.einsum(scheme, *ops, optimize=path[0])\n",
    "    #print(\"Trace = {}.\".format(trace))\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, 2, 3), (2, 3, 4, 5), (4, 5, 6, 7), (6, 7, 8, 9), (8, 9, 10, 11), (10, 11, 12, 13), (12, 13, 14, 15), (14, 15, 16, 17), (16, 17, 18, 19), (18, 19, 20, 21), (20, 21, 22, 23), (22, 23, 0, 1)]\n"
     ]
    }
   ],
   "source": [
    "N = 24\n",
    "dims = 4\n",
    "bond = 4\n",
    "# create some numpy tensors\n",
    "tensors = [np.random.rand(*[bond]*dims) for __ in range(0, N, 2)]\n",
    "# labels should ensure that there is no free index.\n",
    "labels = [tuple([(i + j) % N for j in range(4)]) for i in range(0, N, 2)]\n",
    "print(labels)\n",
    "# Prepare the dictionary.\n",
    "tndict = [(labels[i], tensors[i]) for i in range(len(labels))]\n",
    "#print([tnop for (lab, tnop) in tndict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contraction scheme: abcd,cdef,efgh,ghij,ijkl,klmn,mnop,opqr,qrst,stuv,uvwx,wxab\n",
      "np.einsum_path('abcd,cdef,efgh,ghij,ijkl,klmn,mnop,opqr,qrst,stuv,uvwx,wxab->', ops[0], ops[1], ops[2], ops[3], ops[4], ops[5], ops[6], ops[7], ops[8], ops[9], ops[10], ops[11], optimize='optimal')\n"
     ]
    }
   ],
   "source": [
    "trace = TensorTrace(tndict, opt=\"optimal\")\n",
    "print(\"Trace = {}.\".format(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimalEinsum(scheme, ops, opt = \"greedy\"):\n",
    "    # Contract a tensor network using einsum supplemented with its optimization tools.\n",
    "    ops_args = \", \".join([(\"ops[%d]\" % d) for d in range(len(ops))])\n",
    "    print(\"Calling np.einsum({}, {}).\".format(scheme, ops_args))\n",
    "    path = eval(\"np.einsum_path(\\'%s\\', %s, optimize=\\'%s\\')\" % (scheme, ops_args, opt))\n",
    "    print(\"Contraction process\\n{}: {}\\n{}\".format(path[0][0], path[0][1:], path[1]))\n",
    "    prod = np.einsum(scheme, *ops, optimize=path[0])\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorTranspose(tensor):\n",
    "    # Transpose the tensor, in other words, exchange its row and column indices.\n",
    "    # Note that when we reshape a matrix into (D, D, ..., D) tensor, it stores the indices as as\n",
    "    # row_1, row_2, row_3, ..., row_(D/2), col_1, ..., col_(D/2).\n",
    "    rows = range(0, tensor.ndim//2)\n",
    "    cols = range(tensor.ndim//2, tensor.ndim)\n",
    "    tp_indices = np.concatenate((cols, rows))\n",
    "    return np.transpose(tensor, tp_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ?= A.T is True\n"
     ]
    }
   ],
   "source": [
    "# Testing TensorTranspose\n",
    "nq = 10\n",
    "tensor = np.reshape(np.random.rand(2**nq, 2**dims), [2, 2]*nq)\n",
    "tp_tensor = TensorTranspose(tensor)\n",
    "#print(\"tensor\\n{}\\nand its transpose\\n{}\".format(tensor.reshape(2**dims, 2**dims), tp_tensor.reshape(2**dims, 2**dims)))\n",
    "print(\"A ?= A.T is {}\".format(np.allclose(tensor.reshape(2**nq, 2**nq), tp_tensor.reshape(2**nq, 2**nq).T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNQubitPauli(ind, nq):\n",
    "    # Compute the n-qubit Pauli that is at position 'i' in an ordering based on [I, X, Y, Z].\n",
    "    # We will express the input number in base 4^n - 1.\n",
    "    pauli = np.zeros(nq, dtype = np.int)\n",
    "    for i in range(nq):\n",
    "        pauli[i] = ind % 4\n",
    "        ind = int(ind//4)\n",
    "    return pauli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 2, 2])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing GetNQubitPauli\n",
    "GetNQubitPauli(172, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorKron(tn1, tn2):\n",
    "    # Compute the Kronecker product of two tensors A and B.\n",
    "    # This is not equal to np.tensordot(A, B, axis = 0), see: https://stackoverflow.com/questions/52125078/why-does-tensordot-reshape-not-agree-with-kron.\n",
    "    # Note that when we reshape a matrix into (D, D, ..., D) tensor, it stores the indices as as\n",
    "    # row_1, row_2, row_3, ..., row_(D/2), col_1, ..., col_(D/2).\n",
    "    # We will implement Kronecker product using einsum, as\n",
    "    # np.einsum('rA1 rA2 .. rAn, cA1 cA2 .. cAn, rB1 rB2 .. rBn, cB1 cB2 .. cBn -> rA1 rB1 rA2 rB2 .. rAn rBn, cA1 cB1 cA2 cB2 .. cAn cBn', A, B).\n",
    "    if (tn1.ndim != tn2.ndim):\n",
    "        print(\"TensorKron does not work presently for tensors of different dimensions.\")\n",
    "        return None\n",
    "    tn1_rows = [string.printable[10 + i] for i in range(tn1.ndim//2)]\n",
    "    tn1_cols = [string.printable[10 + i] for i in range(tn1.ndim//2, tn1.ndim)]\n",
    "    tn2_rows = [string.printable[10 + tn1.ndim + i] for i in range(tn2.ndim//2)]\n",
    "    tn2_cols = [string.printable[10 + tn1.ndim + i] for i in range(tn2.ndim//2, tn2.ndim)]\n",
    "    #kron_inds = [\"%s%s\" % (tn1_rows[i], tn2_rows[i]) for i in range(tn1.ndim//2)]\n",
    "    #kron_inds += [\"%s%s\" % (tn1_cols[i], tn2_cols[i]) for i in range(tn1.ndim//2)]\n",
    "    kron_inds = [\"%s\" % (tn1_rows[i]) for i in range(tn1.ndim//2)]\n",
    "    kron_inds += [\"%s\" % (tn2_rows[i]) for i in range(tn1.ndim//2)]\n",
    "    kron_inds += [\"%s\" % (tn1_cols[i]) for i in range(tn1.ndim//2)]\n",
    "    kron_inds += [\"%s\" % (tn2_cols[i]) for i in range(tn1.ndim//2)]\n",
    "    scheme = (\"%s%s,%s%s->%s\" % (\"\".join(tn1_rows), \"\".join(tn1_cols), \"\".join(tn2_rows), \"\".join(tn2_cols), \"\".join(kron_inds)))\n",
    "    return OptimalEinsum(scheme, [tn1, tn2], opt = \"greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling np.einsum(abcdefgh,ijklmnop->abcdijklefghmnop, ops[0], ops[1]).\n",
      "Contraction process\n",
      "einsum_path: [(0, 1)]\n",
      "  Complete contraction:  abcdefgh,ijklmnop->abcdijklefghmnop\n",
      "         Naive scaling:  16\n",
      "     Optimized scaling:  16\n",
      "      Naive FLOP count:  6.554e+04\n",
      "  Optimized FLOP count:  6.554e+04\n",
      "   Theoretical speedup:  1.000\n",
      "  Largest intermediate:  6.554e+04 elements\n",
      "--------------------------------------------------------------------------\n",
      "scaling                  current                                remaining\n",
      "--------------------------------------------------------------------------\n",
      "  16    ijklmnop,abcdefgh->abcdijklefghmnop       abcdijklefghmnop->abcdijklefghmnop\n",
      "TensorKron(A, B) ?= A o B is True\n"
     ]
    }
   ],
   "source": [
    "# Testing TensorKron\n",
    "nq = 4\n",
    "tn1 = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "tn2 = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "kn_tensor = TensorKron(tn1, tn2)\n",
    "#print(\"A\\n{}\\nB\\n{}\\nA o B = \\n{}\".format(tn1.reshape(2**nq, 2**nq), tn2.reshape(2**nq, 2**nq), kn_tensor.reshape(4**nq, 4**nq)))\n",
    "print(\"TensorKron(A, B) ?= A o B is {}\".format(np.allclose(kn_tensor.reshape(4**nq, 4**nq), np.kron(tn1.reshape(2**nq, 2**nq), tn2.reshape(2**nq, 2**nq)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TraceDot(tn1, tn2):\n",
    "    # Compute the trace of the dot product of two tensors A and B.\n",
    "    # If the indices of A are i_0 i_1, ..., i_(2n-1) and that of B are j_0 j_1 ... j_(2n-1)\n",
    "    # then want to contract the indices i_(2k) with j_(2k+1), for all k in [0, n-1].\n",
    "    # While calling np.einsum, we need to ensure that the row index of A is equal to the column index of B.\n",
    "    # Additionally to ensure that we have a trace, we need to match the row and column indices of the product.\n",
    "    tn1_rows = [string.printable[10 + i] for i in range(tn1.ndim//2)]\n",
    "    tn1_cols = [string.printable[10 + i] for i in range(tn1.ndim//2, tn1.ndim)]\n",
    "    # The column indices of tn1 should match row indices of tn2\n",
    "    # So, tn1_cols = tn2_rows.\n",
    "    # the row and column indices of the product must match\n",
    "    # So, tn1_rows = tn2_cols.\n",
    "    scheme = (\"%s%s,%s%s->\" % (\"\".join(tn1_rows), \"\".join(tn1_cols), \"\".join(tn1_cols), \"\".join(tn1_rows)))\n",
    "    return OptimalEinsum(scheme, [tn1, tn2], opt = \"greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling np.einsum(abcdefghij,fghijabcde->, ops[0], ops[1]).\n",
      "Contraction process\n",
      "einsum_path: [(0, 1)]\n",
      "  Complete contraction:  abcdefghij,fghijabcde->\n",
      "         Naive scaling:  10\n",
      "     Optimized scaling:  10\n",
      "      Naive FLOP count:  2.048e+03\n",
      "  Optimized FLOP count:  2.049e+03\n",
      "   Theoretical speedup:  1.000\n",
      "  Largest intermediate:  1.000e+00 elements\n",
      "--------------------------------------------------------------------------\n",
      "scaling                  current                                remaining\n",
      "--------------------------------------------------------------------------\n",
      "  10     fghijabcde,abcdefghij->                                       ->\n",
      "TraceDot(A, B) ?= Tr(A . B) is True.\n"
     ]
    }
   ],
   "source": [
    "# Testing TraceDot\n",
    "nq = 5\n",
    "tn1 = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "tn2 = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "trdot = TraceDot(tn1, tn2)\n",
    "nptr = np.trace(np.dot(tn1.reshape(2**nq, 2**nq), tn2.reshape(2**nq, 2**nq)))\n",
    "#print(\"A\\n{}\\nB\\n{}\\nTr(A . B) = \\n{}\".format(tn1.reshape(2**nq, 2**nq), tn2.reshape(2**nq, 2**nq), trdot))\n",
    "#print(\"TensorTrace = {}\\nNumpy Trace = {}\".format(trdot, nptr))\n",
    "print(\"TraceDot(A, B) ?= Tr(A . B) is {}.\".format(np.allclose(trdot, nptr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KraussToTheta(kraus):\n",
    "    # Convert from the Kraus representation to the \"Theta\" representation.\n",
    "    # The \"Theta\" matrix T of a CPTP map whose chi-matrix is X is defined as:\n",
    "    # T_ij = \\sum_(ij) [ X_ij (P_i o (P_j)^T) ]\n",
    "    # Note that the chi matrix X can be defined using the Kraus matrices {K_k} in the following way\n",
    "    # X_ij = \\sum_k [ <P_i|K_k><K_k|P_j> ]\n",
    "    # \t   = \\sum_k [ Tr(P_i K_k) Tr((K_k)^\\dag P_j)]\n",
    "    # So we find that\n",
    "    # T = \\sum_(ij) [ \\sum_k [ Tr(P_i K_k) Tr((K_k)^\\dag P_j)] ] (P_i o (P_j)^T) ]\n",
    "    # We will store T as a Tensor with dimension = (2 * number of qubits) and bond dimension = 4.\n",
    "    nq = kraus.shape[0]\n",
    "    theta = np.array(*[4, 4]*nq, dtype = np.complex128)\n",
    "    for i in range(4**nq):\n",
    "        for j in range(4**nq):\n",
    "            Pi = get_Pauli_tensor(GetNQubitPauli(i))\n",
    "            Pj = GetNQubitPauli(j)\n",
    "            PjT = TensorTranspose(get_Pauli_tensor(Pj))\n",
    "            PioPjT = TensorKron(Pi, PjT)\n",
    "            coeff = 0 + 0 * 1j\n",
    "            for k in range(kraus.shape[0]):\n",
    "                K = np.reshape(kraus[k, :, :], *[2, 2]*nq)\n",
    "                Kdag = np.conj(TensorTranspose(K))\n",
    "                coeff += TraceDot(Pi, K) * TraceDot(Pj, Kdag)\n",
    "            theta += coeff * PioPjT\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
