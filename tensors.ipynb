{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "from utils import Kron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TupleToString(tup):\n",
    "    # Convert a tuple to a string where each number is assigned to its string form.\n",
    "    return \"\".join(list(map(lambda i: string.printable[10 + i], tup)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YieldScalar(tndict):\n",
    "    # Check if contracting all the tensors in the dictionary yields a scalar.\n",
    "    supports = np.array([np.array(list(support), dtype = np.int) for (support, __) in tndict], dtype = np.int).flatten()\n",
    "    #print(\"supports = {}\".format(supports))\n",
    "    visited = np.zeros(supports.shape[0], dtype = np.int)\n",
    "    for i in range(supports.size):\n",
    "        not_scalar = 1\n",
    "        for j in range(supports.size):\n",
    "            if (i != j):\n",
    "                if (supports[i] == supports[j]):\n",
    "                    not_scalar = 0\n",
    "                    #print(\"Comparing support[%d] = %d and support[%d] = %d.\" % (i, supports[i], j, supports[j])) \n",
    "    scalar = 1 - not_scalar\n",
    "    return scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorTrace(tndict, opt = \"greedy\"):\n",
    "    # Contract a set of tensors provided as a dictionary.\n",
    "    # The result of the traction must be a number, in other words, there should be no free indices.\n",
    "    if (YieldScalar(tndict) == 0):\n",
    "        print(\"The Tensor contraction does not result in a trace.\")\n",
    "        return None\n",
    "    # We want to use np.einsum(...) to perform the Tensor contraction.\n",
    "    # Every element of the input dictionary is a tuple, containing the support of a tensor and its operator form.\n",
    "    # Numpy's einsum function simply needs the Tensors and the corresponding labels associated to its indices.\n",
    "    # We will convert the support of each Tensor to a string, that serves as its label.\n",
    "    scheme = \",\".join([TupleToString(support) for (support, __) in tndict])\n",
    "    print(\"Contraction scheme: {}\".format(scheme))\n",
    "    ops = [op for (__, op) in tndict]\n",
    "    ops_args = \", \".join([(\"ops[%d]\" % d) for d in range(len(ops))])\n",
    "    print(\"np.einsum_path(\\'%s->\\', %s, optimize=\\'%s\\')\" % (scheme, ops_args, opt))\n",
    "    path = eval(\"np.einsum_path(\\'%s->\\', %s, optimize=\\'%s\\')\" % (scheme, ops_args, opt))\n",
    "    print(\"Contraction process\\n{}: {}\\n{}\".format(path[0][0], path[0][1:], path[1]))\n",
    "    trace = np.einsum(scheme, *ops, optimize=path[0])\n",
    "    #print(\"Trace = {}.\".format(trace))\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1, 2, 3), (2, 3, 4, 5), (4, 5, 6, 7), (6, 7, 8, 9), (8, 9, 10, 11), (10, 11, 12, 13), (12, 13, 14, 15), (14, 15, 16, 17), (16, 17, 18, 19), (18, 19, 20, 21), (20, 21, 22, 23), (22, 23, 0, 1)]\n"
     ]
    }
   ],
   "source": [
    "N = 24\n",
    "dims = 4\n",
    "bond = 4\n",
    "# create some numpy tensors\n",
    "tensors = [np.random.rand(*[bond]*dims) for __ in range(0, N, 2)]\n",
    "# labels should ensure that there is no free index.\n",
    "labels = [tuple([(i + j) % N for j in range(4)]) for i in range(0, N, 2)]\n",
    "print(labels)\n",
    "# Prepare the dictionary.\n",
    "tndict = [(labels[i], tensors[i]) for i in range(len(labels))]\n",
    "#print([tnop for (lab, tnop) in tndict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contraction scheme: abcd,cdef,efgh,ghij,ijkl,klmn,mnop,opqr,qrst,stuv,uvwx,wxab\n",
      "np.einsum_path('abcd,cdef,efgh,ghij,ijkl,klmn,mnop,opqr,qrst,stuv,uvwx,wxab->', ops[0], ops[1], ops[2], ops[3], ops[4], ops[5], ops[6], ops[7], ops[8], ops[9], ops[10], ops[11], optimize='optimal')\n"
     ]
    }
   ],
   "source": [
    "trace = TensorTrace(tndict, opt=\"optimal\")\n",
    "print(\"Trace = {}.\".format(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimalEinsum(scheme, ops, opt = \"greedy\"):\n",
    "    # Contract a tensor network using einsum supplemented with its optimization tools.\n",
    "    ops_args = \", \".join([(\"ops[%d]\" % d) for d in range(len(ops))])\n",
    "    print(\"Calling np.einsum({}, {}).\".format(scheme, ops_args))\n",
    "    path = eval(\"np.einsum_path(\\'%s\\', %s, optimize=\\'%s\\')\" % (scheme, ops_args, opt))\n",
    "    #print(\"Contraction process\\n{}: {}\\n{}\".format(path[0][0], path[0][1:], path[1]))\n",
    "    prod = np.einsum(scheme, *ops, optimize=path[0])\n",
    "    return prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorTranspose(tensor):\n",
    "    # Transpose the tensor, in other words, exchange its row and column indices.\n",
    "    # Note that when we reshape a matrix into (D, D, ..., D) tensor, it stores the indices as as\n",
    "    # row_1, row_2, row_3, ..., row_(D/2), col_1, ..., col_(D/2).\n",
    "    rows = range(0, tensor.ndim//2)\n",
    "    cols = range(tensor.ndim//2, tensor.ndim)\n",
    "    tp_indices = np.concatenate((cols, rows))\n",
    "    return np.transpose(tensor, tp_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A ?= A.T is True\n"
     ]
    }
   ],
   "source": [
    "# Testing TensorTranspose\n",
    "nq = 2\n",
    "tensor = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "tp_tensor = TensorTranspose(tensor)\n",
    "#print(\"tensor\\n{}\\nand its transpose\\n{}\".format(tensor.reshape(2**dims, 2**dims), tp_tensor.reshape(2**dims, 2**dims)))\n",
    "print(\"A ?= A.T is {}\".format(np.allclose(tensor.reshape(2**nq, 2**nq), tp_tensor.reshape(2**nq, 2**nq).T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNQubitPauli(ind, nq):\n",
    "    # Compute the n-qubit Pauli that is at position 'i' in an ordering based on [I, X, Y, Z].\n",
    "    # We will express the input number in base 4^n - 1.\n",
    "    pauli = np.zeros(nq, dtype = np.int)\n",
    "    for i in range(nq):\n",
    "        pauli[i] = ind % 4\n",
    "        ind = int(ind//4)\n",
    "    return pauli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing GetNQubitPauli\n",
    "GetNQubitPauli(172, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorKron(tn1, tn2):\n",
    "    # Compute the Kronecker product of two tensors A and B.\n",
    "    # This is not equal to np.tensordot(A, B, axis = 0), see: https://stackoverflow.com/questions/52125078/why-does-tensordot-reshape-not-agree-with-kron.\n",
    "    # Note that when we reshape a matrix into (D, D, ..., D) tensor, it stores the indices as as\n",
    "    # row_1, row_2, row_3, ..., row_(D/2), col_1, ..., col_(D/2).\n",
    "    # We will implement Kronecker product using einsum, as\n",
    "    # np.einsum('rA1 rA2 .. rAn, cA1 cA2 .. cAn, rB1 rB2 .. rBn, cB1 cB2 .. cBn -> rA1 rB1 rA2 rB2 .. rAn rBn, cA1 cB1 cA2 cB2 .. cAn cBn', A, B).\n",
    "    if (tn1.ndim != tn2.ndim):\n",
    "        print(\"TensorKron does not work presently for tensors of different dimensions.\")\n",
    "        return None\n",
    "    tn1_rows = [string.printable[10 + i] for i in range(tn1.ndim//2)]\n",
    "    tn1_cols = [string.printable[10 + i] for i in range(tn1.ndim//2, tn1.ndim)]\n",
    "    tn2_rows = [string.printable[10 + tn1.ndim + i] for i in range(tn2.ndim//2)]\n",
    "    tn2_cols = [string.printable[10 + tn1.ndim + i] for i in range(tn2.ndim//2, tn2.ndim)]\n",
    "    #kron_inds = [\"%s%s\" % (tn1_rows[i], tn2_rows[i]) for i in range(tn1.ndim//2)]\n",
    "    #kron_inds += [\"%s%s\" % (tn1_cols[i], tn2_cols[i]) for i in range(tn1.ndim//2)]\n",
    "    kron_inds = [\"%s\" % (tn1_rows[i]) for i in range(tn1.ndim//2)]\n",
    "    kron_inds += [\"%s\" % (tn2_rows[i]) for i in range(tn1.ndim//2)]\n",
    "    kron_inds += [\"%s\" % (tn1_cols[i]) for i in range(tn1.ndim//2)]\n",
    "    kron_inds += [\"%s\" % (tn2_cols[i]) for i in range(tn1.ndim//2)]\n",
    "    scheme = (\"%s%s,%s%s->%s\" % (\"\".join(tn1_rows), \"\".join(tn1_cols), \"\".join(tn2_rows), \"\".join(tn2_cols), \"\".join(kron_inds)))\n",
    "    return OptimalEinsum(scheme, [tn1, tn2], opt = \"greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling np.einsum(abcdefgh,ijklmnop->abcdijklefghmnop, ops[0], ops[1]).\n",
      "Contraction process\n",
      "einsum_path: [(0, 1)]\n",
      "  Complete contraction:  abcdefgh,ijklmnop->abcdijklefghmnop\n",
      "         Naive scaling:  16\n",
      "     Optimized scaling:  16\n",
      "      Naive FLOP count:  6.554e+04\n",
      "  Optimized FLOP count:  6.554e+04\n",
      "   Theoretical speedup:  1.000\n",
      "  Largest intermediate:  6.554e+04 elements\n",
      "--------------------------------------------------------------------------\n",
      "scaling                  current                                remaining\n",
      "--------------------------------------------------------------------------\n",
      "  16    ijklmnop,abcdefgh->abcdijklefghmnop       abcdijklefghmnop->abcdijklefghmnop\n",
      "TensorKron(A, B) ?= A o B is True\n"
     ]
    }
   ],
   "source": [
    "# Testing TensorKron\n",
    "nq = 4\n",
    "tn1 = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "tn2 = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "kn_tensor = TensorKron(tn1, tn2)\n",
    "#print(\"A\\n{}\\nB\\n{}\\nA o B = \\n{}\".format(tn1.reshape(2**nq, 2**nq), tn2.reshape(2**nq, 2**nq), kn_tensor.reshape(4**nq, 4**nq)))\n",
    "print(\"TensorKron(A, B) ?= A o B is {}\".format(np.allclose(kn_tensor.reshape(4**nq, 4**nq), np.kron(tn1.reshape(2**nq, 2**nq), tn2.reshape(2**nq, 2**nq)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TraceDot(tn1, tn2):\n",
    "    # Compute the trace of the dot product of two tensors A and B.\n",
    "    # If the indices of A are i_0 i_1, ..., i_(2n-1) and that of B are j_0 j_1 ... j_(2n-1)\n",
    "    # then want to contract the indices i_(2k) with j_(2k+1), for all k in [0, n-1].\n",
    "    # While calling np.einsum, we need to ensure that the row index of A is equal to the column index of B.\n",
    "    # Additionally to ensure that we have a trace, we need to match the row and column indices of the product.\n",
    "    tn1_rows = [string.printable[10 + i] for i in range(tn1.ndim//2)]\n",
    "    tn1_cols = [string.printable[10 + i] for i in range(tn1.ndim//2, tn1.ndim)]\n",
    "    # The column indices of tn1 should match row indices of tn2\n",
    "    # So, tn1_cols = tn2_rows.\n",
    "    # the row and column indices of the product must match\n",
    "    # So, tn1_rows = tn2_cols.\n",
    "    scheme = (\"%s%s,%s%s->\" % (\"\".join(tn1_rows), \"\".join(tn1_cols), \"\".join(tn1_cols), \"\".join(tn1_rows)))\n",
    "    return OptimalEinsum(scheme, [tn1, tn2], opt = \"greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling np.einsum(abcdefghij,fghijabcde->, ops[0], ops[1]).\n",
      "Contraction process\n",
      "einsum_path: [(0, 1)]\n",
      "  Complete contraction:  abcdefghij,fghijabcde->\n",
      "         Naive scaling:  10\n",
      "     Optimized scaling:  10\n",
      "      Naive FLOP count:  2.048e+03\n",
      "  Optimized FLOP count:  2.049e+03\n",
      "   Theoretical speedup:  1.000\n",
      "  Largest intermediate:  1.000e+00 elements\n",
      "--------------------------------------------------------------------------\n",
      "scaling                  current                                remaining\n",
      "--------------------------------------------------------------------------\n",
      "  10     fghijabcde,abcdefghij->                                       ->\n",
      "TraceDot(A, B) ?= Tr(A . B) is True.\n"
     ]
    }
   ],
   "source": [
    "# Testing TraceDot\n",
    "nq = 5\n",
    "tn1 = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "tn2 = np.reshape(np.random.rand(2**nq, 2**nq), [2, 2]*nq)\n",
    "trdot = TraceDot(tn1, tn2)\n",
    "nptr = np.trace(np.dot(tn1.reshape(2**nq, 2**nq), tn2.reshape(2**nq, 2**nq)))\n",
    "#print(\"A\\n{}\\nB\\n{}\\nTr(A . B) = \\n{}\".format(tn1.reshape(2**nq, 2**nq), tn2.reshape(2**nq, 2**nq), trdot))\n",
    "#print(\"TensorTrace = {}\\nNumpy Trace = {}\".format(trdot, nptr))\n",
    "print(\"TraceDot(A, B) ?= Tr(A . B) is {}.\".format(np.allclose(trdot, nptr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PauliTensor(pauli_op):\n",
    "    # Convert a Pauli in operator form to a tensor.\n",
    "    # The tensor product of A B .. Z is given by simply putting the rows indices of A, B, ..., Z together, followed by their column indices.\n",
    "    # Each qubit index q can be assigned a pair of labels for the row and columns of the Pauli matrix on q: C[2q], C[2q + 1].\n",
    "    # Pauli matrices\n",
    "    characters = string.printable[10:]\n",
    "    # replace the following line with the variable from globalvars.py\n",
    "    Pauli = np.array([[[1, 0], [0, 1]], [[0, 1], [1, 0]], [[0, -1j], [1j, 0]], [[1, 0], [0, -1]]], dtype=np.complex128)\n",
    "    nq = pauli_op.shape[0]\n",
    "    labels = \",\".join([\"%s%s\" % (characters[2 * q], characters[2 * q + 1]) for q in range(nq)])\n",
    "    ops = [Pauli[pauli_op[q], :, :] for q in range(nq)]\n",
    "    kn_indices = [\"%s\" % (characters[2 * q]) for q in range(nq)]\n",
    "    kn_indices += [\"%s\" % (characters[2 * q + 1]) for q in range(nq)]\n",
    "    kn_label = \"\".join(kn_indices)\n",
    "    scheme = \"%s->%s\" % (labels, kn_label)\n",
    "    pauli_tensor = OptimalEinsum(scheme, ops)\n",
    "    return pauli_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pauli operator: [2 3 3]\n",
      "Calling np.einsum(ab,cd,ef->acebdf, ops[0], ops[1], ops[2]).\n",
      "Contraction process\n",
      "einsum_path: [(0, 1, 2)]\n",
      "  Complete contraction:  ab,cd,ef->acebdf\n",
      "         Naive scaling:  6\n",
      "     Optimized scaling:  6\n",
      "      Naive FLOP count:  1.280e+02\n",
      "  Optimized FLOP count:  1.290e+02\n",
      "   Theoretical speedup:  0.992\n",
      "  Largest intermediate:  6.400e+01 elements\n",
      "--------------------------------------------------------------------------\n",
      "scaling                  current                                remaining\n",
      "--------------------------------------------------------------------------\n",
      "   6            ef,cd,ab->acebdf                           acebdf->acebdf\n",
      "PauliTensor - Numpy = True\n"
     ]
    }
   ],
   "source": [
    "N = 3\n",
    "pauli_op = np.random.randint(0, high=4, size=(N,))\n",
    "print(\"Pauli operator: {}\".format(pauli_op))\n",
    "tn_pauli = PauliTensor(pauli_op)\n",
    "Pauli = np.array([[[1, 0], [0, 1]], [[0, 1], [1, 0]], [[0, -1j], [1j, 0]], [[1, 0], [0, -1]]], dtype=np.complex128)\n",
    "np_pauli = Kron(*Pauli[pauli_op, :, :])\n",
    "print(\"PauliTensor - Numpy = {}\".format(np.allclose(tn_pauli.reshape(2**N, 2**N), np_pauli)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KraussToTheta(kraus):\n",
    "    # Convert from the Kraus representation to the \"Theta\" representation.\n",
    "    # The \"Theta\" matrix T of a CPTP map whose chi-matrix is X is defined as:\n",
    "    # T_ij = \\sum_(ij) [ X_ij (P_i o (P_j)^T) ]\n",
    "    # Note that the chi matrix X can be defined using the Kraus matrices {K_k} in the following way\n",
    "    # X_ij = \\sum_k [ <P_i|K_k><K_k|P_j> ]\n",
    "    # \t   = \\sum_k [ Tr(P_i K_k) Tr((K_k)^\\dag P_j)]\n",
    "    # So we find that\n",
    "    # T = \\sum_(ij) [ \\sum_k [ Tr(P_i K_k) Tr((K_k)^\\dag P_j)] ] (P_i o (P_j)^T) ]\n",
    "    # We will store T as a Tensor with dimension = (2 * number of qubits) and bond dimension = 4.\n",
    "    nq = int(np.log2(kraus.shape[1]))\n",
    "    theta = np.zeros(tuple([4, 4]*nq), dtype = np.complex128)\n",
    "    for i in range(4**nq):\n",
    "        for j in range(4**nq):\n",
    "            Pi = PauliTensor(GetNQubitPauli(i, nq))\n",
    "            Pj = GetNQubitPauli(j, nq)\n",
    "            PjT = TensorTranspose(PauliTensor(Pj))\n",
    "            PioPjT = np.reshape(TensorKron(Pi, PjT), tuple([4, 4] * nq))\n",
    "            print(\"Pi.shape = {}\".format(Pi.shape))\n",
    "            coeff = 0 + 0 * 1j\n",
    "            for k in range(kraus.shape[0]):\n",
    "                K = np.reshape(kraus[k, :, :], tuple([2, 2]*nq))\n",
    "                Kdag = np.conj(TensorTranspose(K))\n",
    "                print(\"K.shape = {}\".format(K.shape))\n",
    "                coeff += TraceDot(Pi, K) * TraceDot(Pj, Kdag)\n",
    "            theta += coeff * PioPjT\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling np.einsum(ab->ab, ops[0]).\n",
      "Calling np.einsum(ab->ab, ops[0]).\n",
      "Calling np.einsum(ab,cd->acbd, ops[0], ops[1]).\n",
      "Pi.shape = (2, 2)\n",
      "K.shape = (2, 2)\n",
      "Calling np.einsum(ab,ba->, ops[0], ops[1]).\n",
      "Calling np.einsum(a,a->, ops[0], ops[1]).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Einstein sum subscript , does not contain the correct number of indices for operand 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-76d92dbfac92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mkraus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mKraussToTheta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkraus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-ef733c40e366>\u001b[0m in \u001b[0;36mKraussToTheta\u001b[1;34m(kraus)\u001b[0m\n\u001b[0;32m     23\u001b[0m                 \u001b[0mKdag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensorTranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"K.shape = {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[0mcoeff\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mTraceDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mTraceDot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKdag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mtheta\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcoeff\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mPioPjT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-35838ec5fea4>\u001b[0m in \u001b[0;36mTraceDot\u001b[1;34m(tn1, tn2)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# So, tn1_rows = tn2_cols.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mscheme\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"%s%s,%s%s->\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtn1_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtn1_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtn1_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtn1_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mOptimalEinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtn1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtn2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"greedy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-dae747d50722>\u001b[0m in \u001b[0;36mOptimalEinsum\u001b[1;34m(scheme, ops, opt)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mops_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\", \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ops[%d]\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Calling np.einsum({}, {}).\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"np.einsum_path(\\'%s\\', %s, optimize=\\'%s\\')\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m#print(\"Contraction process\\n{}: {}\\n{}\".format(path[0][0], path[0][1:], path[1]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscheme\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36meinsum_path\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\einsumfunc.py\u001b[0m in \u001b[0;36meinsum_path\u001b[1;34m(*operands, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m             raise ValueError(\"Einstein sum subscript %s does not contain the \"\n\u001b[0;32m    868\u001b[0m                              \u001b[1;34m\"correct number of indices for operand %d.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 869\u001b[1;33m                              % (input_subscripts[tnum], tnum))\n\u001b[0m\u001b[0;32m    870\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m             \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcnum\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Einstein sum subscript , does not contain the correct number of indices for operand 1."
     ]
    }
   ],
   "source": [
    "N = 1\n",
    "kraus = np.random.rand(4**N, 2**N, 2**N)\n",
    "KraussToTheta(kraus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SupportToLabel(support, characters = None):\n",
    "\t# Convert a list of qubit indices to labels for a tensor.\n",
    "\t# Each qubit index corresponds to a pair of labels, indicating the row and column indices of the 2 x 2 matrix which acts non-trivially on that qubit.\n",
    "\t# Each number in the support is mapped to a pair of alphabets in the characters list, as: x -> (characters[2x], characters[2x + 1]).\n",
    "\t# Eg. (x, y, z) ---> (C[2x] C[2y] C[2z] , C[2x + 1] C[2y + 1] C[2z + 1])\n",
    "\tif characters == None:\n",
    "\t\tcharacters = string.printable[10:]\n",
    "\tlabel = ['0' for __ in range(2 * len(support))]\n",
    "\tnq = len(support)\n",
    "\tfor s in range(nq):\n",
    "\t\tlabel[s] = characters[2 * support[s]]\n",
    "\t\tlabel[nq + s] = characters[2 * support[s] + 1]\n",
    "\treturn label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ContractThetaNetwork(theta_dict):\n",
    "\t# Compute the Theta matrix of a composition of channels.\n",
    "\t# The individual channels are provided a list where each one is a pair: (s, O) where s is the support and O is the theta matrix.\n",
    "\t# We will use einsum to contract the tensor network of channels.\n",
    "\tsupports = np.array([sup for (sup, op) in theta_dict], dtype = np.int).flatten()\n",
    "\tlabels = \",\".join([\"\".join(SupportToLabel(sup)) for (sup, op) in theta_dict])\n",
    "\t(__, order) = np.unique(supports, return_indices=True)\n",
    "\tcomposed_support = supports[np.argsort(order)]\n",
    "\tcomposed_label = \"\".join(SupportToLabel(composed_support))\n",
    "\tcontraction_scheme = \"%s->%s\" % (labels, composed_label)\n",
    "\ttheta_ops = [op for (__, op) in theta_dict]\n",
    "\tcomposed = OptimalEinsum(contraction_scheme, theta_ops)\n",
    "\treturn composed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
